@article{Arya2004LocalSH,
  title = "A local search approximation algorithm for k-means clustering",
journal = "Computational Geometry",
volume = "28",
number = "2",
pages = "89 - 112",
year = "2004",
note = "Special Issue on the 18th Annual Symposium on Computational Geometry",
issn = "0925-7721",
doi = "https://doi.org/10.1016/j.comgeo.2004.03.003",
url = "http://www.sciencedirect.com/science/article/pii/S0925772104000215",
author = "Tapas Kanungo and David M. Mount and Nathan S. Netanyahu and Christine D. Piatko and Ruth Silverman and Angela Y. Wu",
}
@inproceedings{10.1145/62212.62255,
author = {Feder, Tom\'{a}s and Greene, Daniel},
title = {Optimal Algorithms for Approximate Clustering},
year = {1988},
isbn = {0897912640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/62212.62255},
doi = {10.1145/62212.62255},
booktitle = {Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing},
pages = {434–444},
numpages = {11},
location = {Chicago, Illinois, USA},
series = {STOC ’88}
}
@article{10.5555/3116656.3116964,
author = {Har-Peled, Sariel},
title = {Clustering Motion},
year = {2004},
issue_date = {March     2004},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {4},
issn = {0179-5376},
abstract = {   Given a set of moving points in undefinedd, we show how to   cluster them in advance, using a small number of   clusters, so that at any time this static clustering is   competitive with the optimal k-center clustering at   that time. The advantage of this approach is that it   avoids updating the clustering as time passes. We also   show how to maintain this static clustering efficiently   under insertions and deletions.     To implement this static clustering efficiently, we   describe a simple technique for speeding up clustering   algorithms and apply it to achieve faster clustering   algorithms for several problems. In particular, we   present a linear time algorithm for computing a   2-approximation to the k-center clustering of a set of   n points in undefinedd. This slightly improves the   algorithm of Feder and Greene, that runs in undefined(n   log k) time (which is optimal in the algebraic   decision tree model). },
journal = {Discrete Comput. Geom.},
month = mar,
pages = {545–565},
numpages = {21}
}

@article{doi:10.1137/S0036144599352836,
author = {Du, Qiang and Faber, Vance and Gunzburger, Max},
title = {Centroidal Voronoi Tessellations: Applications and Algorithms},
journal = {SIAM Review},
volume = {41},
number = {4},
pages = {637-676},
year = {1999},
doi = {10.1137/S0036144599352836},

URL = { 
        https://doi.org/10.1137/S0036144599352836
    
},
eprint = { 
        https://doi.org/10.1137/S0036144599352836
    
}

}
@article{LI2001516,
title = "Improved Bounds on the Sample Complexity of Learning",
journal = "Journal of Computer and System Sciences",
volume = "62",
number = "3",
pages = "516 - 527",
year = "2001",
issn = "0022-0000",
doi = "https://doi.org/10.1006/jcss.2000.1741",
url = "http://www.sciencedirect.com/science/article/pii/S0022000000917410",
author = "Yi Li and Philip M. Long and Aravind Srinivasan",
keywords = "sample complexity, machine learning, empirical process theory, PAC learning, agnostic learning",
abstract = "We present a new general upper bound on the number of examples required to estimate all of the expectations of a set of random variables uniformly well. The quality of the estimates is measured using a variant of the relative error proposed by Haussler and Pollard. We also show that our bound is within a constant factor of the best possible. Our upper bound implies improved bounds on the sample complexity of learning according to Haussler's decision theoretic model."
}

@ARTICLE{1056489,
  author={S. {Lloyd}},
  journal={IEEE Transactions on Information Theory}, 
  title={Least squares quantization in PCM}, 
  year={1982},
  volume={28},
  number={2},
  pages={129-137},}

@article{article,
author = {Aloise, Daniel and Deshpande, Amit and Hansen, Pierre and Popat, Preyas},
year = {2009},
month = {05},
pages = {245-248},
title = {NP-hardness of Euclidean sum-of-squares clustering},
volume = {75},
journal = {Machine Learning},
doi = {10.1007/s10994-009-5103-0}
}

@article{10.1145/293347.293348,
author = {Arya, Sunil and Mount, David M. and Netanyahu, Nathan S. and Silverman, Ruth and Wu, Angela Y.},
title = {An Optimal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions},
year = {1998},
issue_date = {Nov. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {6},
issn = {0004-5411},
url = {https://doi.org/10.1145/293347.293348},
doi = {10.1145/293347.293348},
journal = {J. ACM},
month = nov,
pages = {891–923},
numpages = {33},
keywords = {box-decomposition trees, approximation algorithms, closet-point queries, priority search, post-office problem, nearest neighbor searching}
}

@book{pisier_1989, 
place={Cambridge}, 
series={Cambridge Tracts in Mathematics}, 
title={The Volume of Convex Bodies and Banach Space Geometry}, 
DOI={10.1017/CBO9780511662454}, 
publisher={Cambridge University Press},
author={Pisier, Gilles}, 
year={1989},
collection={Cambridge Tracts in Mathematics}}

@book{chazelle_2000, 
place={Cambridge}, 
title={The Discrepancy Method: Randomness and Complexity}, 
DOI={10.1017/CBO9780511626371}, 
publisher={Cambridge University Press}, 
author={Chazelle, Bernard},
year={2000}}
  
@inproceedings{10.1145/380752.380755,
author = {Arya, Vijay and Garg, Naveen and Khandekar, Rohit and Meyerson, Adam and Munagala, Kamesh and Pandit, Vinayaka},
title = {Local Search Heuristic for K-Median and Facility Location Problems},
year = {2001},
isbn = {1581133499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/380752.380755},
doi = {10.1145/380752.380755},
booktitle = {Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing},
pages = {21–29},
numpages = {9},
location = {Hersonissos, Greece},
series = {STOC ’01}
}
  


@article{Gonzalez1985ClusteringTM,
  title={Clustering to Minimize the Maximum Intercluster Distance},
  author={Teofilo F. Gonzalez},
  journal={Theor. Comput. Sci.},
  year={1985},
  volume={38},
  pages={293-306}
}
@article{DBLP:journals/ki/MunteanuS18,
  author    = {Alexander Munteanu and
               Chris Schwiegelshohn},
  title     = {Coresets-Methods and History: {A} Theoreticians Design Pattern for
               Approximation and Streaming Algorithms},
  journal   = {K{\"{u}}nstliche Intell.},
  volume    = {32},
  number    = {1},
  pages     = {37--53},
  year      = {2018},
  url       = {https://doi.org/10.1007/s13218-017-0519-3},
  doi       = {10.1007/s13218-017-0519-3},
  timestamp = {Tue, 14 Jul 2020 14:26:43 +0200},
  biburl    = {https://dblp.org/rec/journals/ki/MunteanuS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{bachem2017scalable,
    title={Scalable k-Means Clustering via Lightweight Coresets},
    author={Olivier Bachem and Mario Lucic and Andreas Krause},
    year={2017},
    eprint={1702.08248},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{10.1145/1007352.1007400,
author = {Har-Peled, Sariel and Mazumdar, Soham},
title = {On Coresets for K-Means and k-Median Clustering},
year = {2004},
isbn = {1581138520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007352.1007400},
doi = {10.1145/1007352.1007400},
abstract = {In this paper, we show the existence of small coresets for the problems of computing k-median and k-means clustering for points in low dimension. In other words, we show that given a point set P in Rd, one can compute a weighted set S ⊆ P, of size O(k ε-d log n), such that one can compute the k-median/means clustering on S instead of on P, and get an (1+ε)-approximation. As a result, we improve the fastest known algorithms for (1+ε)-approximate k-means and k-median. Our algorithms have linear running time for a fixed k and ε. In addition, we can maintain the (1+ε)-approximate k-median or k-means clustering of a stream when points are being only inserted, using polylogarithmic space and update time.},
booktitle = {Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing},
pages = {291–300},
numpages = {10},
keywords = {k-means, clustering, Coreset, streaming, k-median},
location = {Chicago, IL, USA},
series = {STOC '04}
}

@article{Matousek99onapproximate,
    author = {Jirí Matousek},
    title = {On Approximate Geometric K-Clustering},
    journal={Discrete and Computational Geometry},
    year={2000},
    volume={24},
    pages={61-84}
}
  